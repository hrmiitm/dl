[
  {
    "objectID": "Week/Week2/taylor_gd.html",
    "href": "Week/Week2/taylor_gd.html",
    "title": "Taylor Series",
    "section": "",
    "text": "The Taylor series is a way to approximate a function around a known point (a) as:\n\\[\\begin{align*}\n  f(x) = f(a) + \\frac{f'(a)}{1!}(x - a) + \\frac{f''(a)}{2!}(x - a)^2 + \\cdots ...\n\\end{align*}\\]\nWhere:\n\na is a known value.\nx is the value we want to approximate.\n\n\n\n\n\\(\\sqrt{36} = 6\\) , a=36\n\\(\\sqrt{35} = ?\\), x=35\n\\(f(x) = \\sqrt{x}\\)\n\\(f'(x) = \\frac{1}{2*\\sqrt{x}}\\)\n\nUsing a linear approximation (first-order Taylor expansion):\n\\[\\begin{align*}\nf(35) \\approx f(36) + f'(36)(35 - 36)\n\n= 6 + \\frac{1}{2*\\sqrt{36}} * (-1)\n\n= 5.916\n\\end{align*}\\]",
    "crumbs": [
      "Week2",
      "Taylor Series"
    ]
  },
  {
    "objectID": "Week/Week2/taylor_gd.html#taylor-series",
    "href": "Week/Week2/taylor_gd.html#taylor-series",
    "title": "Taylor Series",
    "section": "",
    "text": "The Taylor series is a way to approximate a function around a known point (a) as:\n\\[\\begin{align*}\n  f(x) = f(a) + \\frac{f'(a)}{1!}(x - a) + \\frac{f''(a)}{2!}(x - a)^2 + \\cdots ...\n\\end{align*}\\]\nWhere:\n\na is a known value.\nx is the value we want to approximate.\n\n\n\n\n\\(\\sqrt{36} = 6\\) , a=36\n\\(\\sqrt{35} = ?\\), x=35\n\\(f(x) = \\sqrt{x}\\)\n\\(f'(x) = \\frac{1}{2*\\sqrt{x}}\\)\n\nUsing a linear approximation (first-order Taylor expansion):\n\\[\\begin{align*}\nf(35) \\approx f(36) + f'(36)(35 - 36)\n\n= 6 + \\frac{1}{2*\\sqrt{36}} * (-1)\n\n= 5.916\n\\end{align*}\\]",
    "crumbs": [
      "Week2",
      "Taylor Series"
    ]
  },
  {
    "objectID": "Week/Week2/represent_boolean.html",
    "href": "Week/Week2/represent_boolean.html",
    "title": "Boolean Implementation",
    "section": "",
    "text": "Can we implement any Boolean function with single perceptron?\n\n\nAnswer\n\nAnswer: No, Single Perceptron can only separate Linearly Separable Inputs.\n\nCan non linearly separable boolean function be implemented with multiple perceptron? How?\n\n\nAnswer\n\nAnswer: Yes, with \\(2^n + 1\\) perceptron and 3 layers(input, hidden(\\(2^n\\)), output(1)) we can definetely implement. Note: With less than \\(2^n+1\\), is also possible\n\nDo you have intution that how perceptron that linearly separate is now able to separate even non-linearly seprable boolean inputs?\n\n\nAnswer\n\nAnswer: If No then watch my summary video.\n\nCan non linearly separable boolean function be implemented with multiple perceptron? How?\n\n\nAnswer\n\nAnswer: Yes, with \\(2^n + 1\\) perceptron and 3 layers(input, hidden(\\(2^n\\)), output(1)) we can definetely implement. Note: With less than \\(2^n+1\\), is also possible\n\nWhat is minimum no. of neuron are required to implement XOR boolean function?\n\n\nAnswer\n\nAnswer: 3, with \\(2^2+1 = 5\\) we can definitely do, but question asks for minimum, and with just 3 we can implement boolean function.",
    "crumbs": [
      "Week2",
      "Boolean Implementation"
    ]
  },
  {
    "objectID": "Week/Week2/represent_boolean.html#questions",
    "href": "Week/Week2/represent_boolean.html#questions",
    "title": "Boolean Implementation",
    "section": "",
    "text": "Can we implement any Boolean function with single perceptron?\n\n\nAnswer\n\nAnswer: No, Single Perceptron can only separate Linearly Separable Inputs.\n\nCan non linearly separable boolean function be implemented with multiple perceptron? How?\n\n\nAnswer\n\nAnswer: Yes, with \\(2^n + 1\\) perceptron and 3 layers(input, hidden(\\(2^n\\)), output(1)) we can definetely implement. Note: With less than \\(2^n+1\\), is also possible\n\nDo you have intution that how perceptron that linearly separate is now able to separate even non-linearly seprable boolean inputs?\n\n\nAnswer\n\nAnswer: If No then watch my summary video.\n\nCan non linearly separable boolean function be implemented with multiple perceptron? How?\n\n\nAnswer\n\nAnswer: Yes, with \\(2^n + 1\\) perceptron and 3 layers(input, hidden(\\(2^n\\)), output(1)) we can definetely implement. Note: With less than \\(2^n+1\\), is also possible\n\nWhat is minimum no. of neuron are required to implement XOR boolean function?\n\n\nAnswer\n\nAnswer: 3, with \\(2^2+1 = 5\\) we can definitely do, but question asks for minimum, and with just 3 we can implement boolean function.",
    "crumbs": [
      "Week2",
      "Boolean Implementation"
    ]
  },
  {
    "objectID": "Week/Week2/boolean.html",
    "href": "Week/Week2/boolean.html",
    "title": "Boolean Functions",
    "section": "",
    "text": "flowchart LR\n  A[\"Multiple Inputs (0,1)\"] --&gt; B(Processing) --&gt; C[\"Single Output (0,1)\"]\n\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^0\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\n\n\n\n\\(\\phi\\)\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^1\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n\\(f_4\\)\n\n\n\n\n0\n0\n0\n1\n1\n\n\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^2\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n\\(f_4\\)\n\\(f_5\\)\n\\(f_6\\)\n\\(f_7\\)\n\\(f_8\\)\n\\(f_9\\)\n\\(f_{10}\\)\n\\(f_{11}\\)\n\\(f_{12}\\)\n\\(f_{13}\\)\n\\(f_{14}\\)\n\\(f_{15}\\)\n\\(f_{16}\\)\n\n\n\n\n(0, 0)\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n(0, 1)\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n\n\n(1, 0)\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n\n\n(1, 1)\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^3\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n…\n…\n\\(f_{256}\\)\n\n\n\n\n(0, 0, 0)\n0\n0\n0\n…\n…\n1\n\n\n(0, 0, 1)\n0\n0\n1\n…\n…\n1\n\n\n(0, 1, 0)\n0\n1\n0\n…\n…\n1\n\n\n(0, 1, 1)\n0\n1\n1\n…\n…\n1\n\n\n(1, 0, 0)\n1\n0\n0\n…\n…\n1\n\n\n(1, 0, 1)\n1\n0\n1\n…\n…\n1\n\n\n(1, 1, 0)\n1\n1\n0\n…\n…\n1\n\n\n(1, 1, 1)\n1\n1\n1\n…\n…\n1\n\n\n\n\nNote: For Case 4, there are \\(2^{2^3} = 256\\) possible functions, so only a few examples are shown, with ... to indicate the continuation.",
    "crumbs": [
      "Week2",
      "Boolean Functions"
    ]
  },
  {
    "objectID": "Week/Week2/boolean.html#boolean-inputs-and-boolean-output",
    "href": "Week/Week2/boolean.html#boolean-inputs-and-boolean-output",
    "title": "Boolean Functions",
    "section": "",
    "text": "flowchart LR\n  A[\"Multiple Inputs (0,1)\"] --&gt; B(Processing) --&gt; C[\"Single Output (0,1)\"]\n\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^0\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\n\n\n\n\\(\\phi\\)\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^1\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n\\(f_4\\)\n\n\n\n\n0\n0\n0\n1\n1\n\n\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^2\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n\\(f_4\\)\n\\(f_5\\)\n\\(f_6\\)\n\\(f_7\\)\n\\(f_8\\)\n\\(f_9\\)\n\\(f_{10}\\)\n\\(f_{11}\\)\n\\(f_{12}\\)\n\\(f_{13}\\)\n\\(f_{14}\\)\n\\(f_{15}\\)\n\\(f_{16}\\)\n\n\n\n\n(0, 0)\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n(0, 1)\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n\n\n(1, 0)\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n\n\n(1, 1)\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nInput: \\(\\{0,1\\}^3\\)  Output: \\(\\{0,1\\}\\)\n\n\n\n\nInput\n\\(f_1\\)\n\\(f_2\\)\n\\(f_3\\)\n…\n…\n\\(f_{256}\\)\n\n\n\n\n(0, 0, 0)\n0\n0\n0\n…\n…\n1\n\n\n(0, 0, 1)\n0\n0\n1\n…\n…\n1\n\n\n(0, 1, 0)\n0\n1\n0\n…\n…\n1\n\n\n(0, 1, 1)\n0\n1\n1\n…\n…\n1\n\n\n(1, 0, 0)\n1\n0\n0\n…\n…\n1\n\n\n(1, 0, 1)\n1\n0\n1\n…\n…\n1\n\n\n(1, 1, 0)\n1\n1\n0\n…\n…\n1\n\n\n(1, 1, 1)\n1\n1\n1\n…\n…\n1\n\n\n\n\nNote: For Case 4, there are \\(2^{2^3} = 256\\) possible functions, so only a few examples are shown, with ... to indicate the continuation.",
    "crumbs": [
      "Week2",
      "Boolean Functions"
    ]
  },
  {
    "objectID": "Week/Week2/boolean.html#no.-of-boolean-functions",
    "href": "Week/Week2/boolean.html#no.-of-boolean-functions",
    "title": "Boolean Functions",
    "section": "No. of Boolean Functions",
    "text": "No. of Boolean Functions\nIf n boolean inputs then \\(2^{2^n}\\) Boolean Functions Possible",
    "crumbs": [
      "Week2",
      "Boolean Functions"
    ]
  },
  {
    "objectID": "Week/Week2/boolean.html#how-many-linearly-seprable",
    "href": "Week/Week2/boolean.html#how-many-linearly-seprable",
    "title": "Boolean Functions",
    "section": "How many Linearly Seprable",
    "text": "How many Linearly Seprable\n\nCase 1: n=0 : All 2\nCase 2: n=1 : All 4\nCase 3: n=2 : \\((16-2) = 14\\), only XOR and !XOR are NonLinearlySeprable\nFor any General n: Manuall check required, no general formula",
    "crumbs": [
      "Week2",
      "Boolean Functions"
    ]
  },
  {
    "objectID": "Week/Week2/boolean.html#question",
    "href": "Week/Week2/boolean.html#question",
    "title": "Boolean Functions",
    "section": "Question",
    "text": "Question\n\nWhy for n boolean inputs there exists \\(2^{2^n}\\) boolean functions ?\n\n\nAnswer\n\nWatch My Summary or Assignment explanation video.",
    "crumbs": [
      "Week2",
      "Boolean Functions"
    ]
  },
  {
    "objectID": "mcculloch_pits.html",
    "href": "mcculloch_pits.html",
    "title": "McCulloch & Pitts",
    "section": "",
    "text": "Artificial neurons are inspired by biological neurons, but there are significant differences between them:\n\n\n\nEfficiency\n\nThe human brain contains approximately 100 billion neurons and consumes about 20 watts of power.\nSimple artificial neural networks can consume power in the range of kilowatts to megawatts, making them far less efficient.\n\nProcessing Mechanism\n\nBiological neurons process signals through complex, largely unknown mechanisms in the soma and other parts of the neuron.\nArtificial neurons process signals using straightforward mathematical functions and weighted sums.\n\n\n\n\n\n\nInspiration\n\nBoth biological and artificial neurons are based on the concept of processing and transmitting signals.\nThey use inputs (stimuli for biological neurons, numerical values for artificial neurons) and generate outputs (responses or activations).\n\nInterconnected Systems\n\nBiological neurons connect through synapses to form neural networks in the brain.\nArtificial neurons are interconnected in layers to form artificial neural networks.\n\nLearning and Adaptation\n\nBiological neurons adapt through synaptic plasticity (e.g., Hebbian learning).\nArtificial neurons adapt by updating weights using algorithms such as gradient descent.\n\n\n\nBy understanding both the differences and similarities, we can better appreciate the capabilities and limitations of artificial neurons compared to their biological counterparts.",
    "crumbs": [
      "Week1",
      "McCulloch & Pitts"
    ]
  },
  {
    "objectID": "mcculloch_pits.html#biological-neuron-vs-artificial-neuron",
    "href": "mcculloch_pits.html#biological-neuron-vs-artificial-neuron",
    "title": "McCulloch & Pitts",
    "section": "",
    "text": "Artificial neurons are inspired by biological neurons, but there are significant differences between them:\n\n\n\nEfficiency\n\nThe human brain contains approximately 100 billion neurons and consumes about 20 watts of power.\nSimple artificial neural networks can consume power in the range of kilowatts to megawatts, making them far less efficient.\n\nProcessing Mechanism\n\nBiological neurons process signals through complex, largely unknown mechanisms in the soma and other parts of the neuron.\nArtificial neurons process signals using straightforward mathematical functions and weighted sums.\n\n\n\n\n\n\nInspiration\n\nBoth biological and artificial neurons are based on the concept of processing and transmitting signals.\nThey use inputs (stimuli for biological neurons, numerical values for artificial neurons) and generate outputs (responses or activations).\n\nInterconnected Systems\n\nBiological neurons connect through synapses to form neural networks in the brain.\nArtificial neurons are interconnected in layers to form artificial neural networks.\n\nLearning and Adaptation\n\nBiological neurons adapt through synaptic plasticity (e.g., Hebbian learning).\nArtificial neurons adapt by updating weights using algorithms such as gradient descent.\n\n\n\nBy understanding both the differences and similarities, we can better appreciate the capabilities and limitations of artificial neurons compared to their biological counterparts.",
    "crumbs": [
      "Week1",
      "McCulloch & Pitts"
    ]
  },
  {
    "objectID": "mcculloch_pits.html#mcculloch-neuroscientist-pitts-logician",
    "href": "mcculloch_pits.html#mcculloch-neuroscientist-pitts-logician",
    "title": "McCulloch & Pitts",
    "section": "McCulloch (Neuroscientist) & Pitts (Logician)",
    "text": "McCulloch (Neuroscientist) & Pitts (Logician)\n\n\n\n\n\nflowchart LR\n  A[\"Inputs (0,1)\"] --&gt; B(Processing) --&gt; C[\"Output (0,1)\"]\n\n\n\n\n\n\n\nTerminology\nSuppose you’re deciding whether a customer will buy your product based on the following factors:\n\nInputs:\n\nisMarketOpen: Whether the market is open.\nhasHusbandHaveInterest: Whether the husband is interested.\nhisWifeHasInterest: Whether the wife is interested.\nveryRichPerson: Whether the customer is very wealthy.\n\nOutput:\n\nThey will buy:\n\nIf both the husband and wife have interest.\nIf anyone has interest and they are a very rich person.\n\nThey will not buy:\n\nIf no one has interest, regardless of whether they are rich or not.\n\n\n\n\n\n1. Inhibitory Input ——\\(\\circ\\)\n\nIf any one of these is active (1), then the output will always be 0.\n\nExample: isMarketOpen is inhibitory input, without that buying is impossible.\n\n\n\n2. Excitatory Input \\(\\longrightarrow\\)\n\nIf any one of these is active (1), it pushes toward an output of 1.\n\nExample: Except isMarketOpen all other 3 are excitatory input.\n\n\n\n3. Threshold\n\nThis is the barrier that needs to be crossed by excitatory input to produce an output of 1.\n\nExample: In above example thresold will be 2.\n\n\n\n\n\nProcessing - Prediction Rule\n\\[\\begin{align*}\n  y =\n    \\begin{cases}\n      0, & \\text{IF any inhibitory input is active} \\\\\n      \\begin{cases}\n        1, & \\text{if } \\sum_{i} x_i \\geq \\text{threshold} \\\\\n        0, & \\text{otherwise}\n      \\end{cases}, & \\text{ELSE}\n    \\end{cases}\n\\end{align*}\\]\nHere, \\(x_i\\) represent excitatory input.\n\n\n\nExample\n\n\n\n\n\nflowchart LR\n  p[hasHusbandHaveIntrest] --&gt; o(((\"MyNeuron Thresold=2 \"))) --&gt; C[\"0 or 1 ?\"]\n  q[hasWifeHaveIntrest] --&gt; o\n  r[isVeryRich] --&gt; o\n\n\n\n\n\n\n\nCase1: Input(0,0,0) —&gt; 0+0+0 &gt;= 0 —&gt; Output = 0\nCase2: Input(0,0,1) —&gt; 0+0+1 &gt;= 1 —&gt; Output = 0\nCase3: Input(0,1,0) —&gt; 0+1+0 &gt;= 1 —&gt; Output = 0\nCase4: Input(1,0,0) —&gt; 1+0+0 &gt;= 1 —&gt; Output = 0\nCase5: Input(1,1,0) —&gt; 1+1+0 &gt;= 2 —&gt; Output = 1\nCase6: Input(1,0,1) —&gt; 1+0+1 &gt;= 2 —&gt; Output = 1\nCase7: Input(0,1,1) —&gt; 0+1+1 &gt;= 2 —&gt; Output = 1\nCase8: Input(1,1,1) —&gt; 1+1+1 &gt;= 3 —&gt; Output = 1\n\n\n\nLimitations\n\nInputs and Outputs are Booleans only.\nEach input has same importances/weightages.\nWork for only linearly seperable cases.",
    "crumbs": [
      "Week1",
      "McCulloch & Pitts"
    ]
  },
  {
    "objectID": "mcculloch_pits.html#questions",
    "href": "mcculloch_pits.html#questions",
    "title": "McCulloch & Pitts",
    "section": "Questions",
    "text": "Questions\n\n\nIs bilogical neuron and artificial neuron is same?\n\n\nAnswer\n\nAnswer: No, Read First Paragraph for Differences\n\nRepresent AND & OR logical gates via McCulloch and Pitts Neuron.\nWhich of the following logical gates cannot be represented by McCulloch Pitts Neuron?\n\nAND\nOR\nNOT\nNOR\nNAND\nXOR\n\n\n\nAnswer\n\nAnswer: XOR, since its is not linearly seperable.\n\n\n\n\n\nThanks, If any disperency or suggestion. Feel free to report me.",
    "crumbs": [
      "Week1",
      "McCulloch & Pitts"
    ]
  },
  {
    "objectID": "classical_perceptron_training.html",
    "href": "classical_perceptron_training.html",
    "title": "Perceptron Training Algorithm",
    "section": "",
    "text": "We now know how perceptron predicts.\nInput is given by the situation, but what will be weights? Weights can be computed by given some input data and labels, and then our perceptron algorithm will learn most appropriate weights.",
    "crumbs": [
      "Week1",
      "Perceptron Training Algorithm"
    ]
  },
  {
    "objectID": "classical_perceptron_training.html#perceptron-learning-algorithm",
    "href": "classical_perceptron_training.html#perceptron-learning-algorithm",
    "title": "Perceptron Training Algorithm",
    "section": "Perceptron Learning Algorithm",
    "text": "Perceptron Learning Algorithm\nGiven data:\n\nInputs: [(2,0), (2,1), (0,1), (-2,0), (-2,-2), (0,-2)]\nOutputs: ([1, 1, 1, 0, 0, 0])\n\n\nSteps to Compute/Learn Weights\n\nInitialize the weight vector randomly e.g. \\(w = (-1,1)\\)\nRepeat until convergence:\n\nPick any input point, say \\(p\\).\nIf the weight vector classifies \\(p\\) incorrectly:\n\nAdjust the weight vector using the rules below.\n\n\n\n\n\nRules to Adjust Weights\n\\[\\begin{align*}\nw_{\\text{new}} =\n\\begin{cases}\n    w + p, & \\text{if } p \\in \\text{+ve(1) and } w^T x &lt; 0, \\\\\n    w - p, & \\text{if } p \\in \\text{-ve(0) and } w^T x \\geq 0, \\\\\n    w, & \\text{else}.\n\\end{cases}\n\\end{align*}\\]\n\n\nCaution\nWhile updating weights etc we should account for bias term also.\n\n\\(w = [1, x_{corr}, y_{corr}]\\)\n\\(x = [1, x_{corr}, y_{corr}]\\)",
    "crumbs": [
      "Week1",
      "Perceptron Training Algorithm"
    ]
  },
  {
    "objectID": "classical_perceptron_training.html#practice",
    "href": "classical_perceptron_training.html#practice",
    "title": "Perceptron Training Algorithm",
    "section": "Practice",
    "text": "Practice\nQuestion) Calculate the weight vector using above mentioned algorithm\n\nGiven data:\n\nInputs: [(2,0), (2,1), (0,1), (-2,0), (-2,-2), (0,-2)]\nOutputs: ([1, 1, 1, 0, 0, 0])\n\n\n\n\nAnswer\n\nAnswer: Watch Youtube Vedio",
    "crumbs": [
      "Week1",
      "Perceptron Training Algorithm"
    ]
  },
  {
    "objectID": "about_dl.html",
    "href": "about_dl.html",
    "title": "About The Course",
    "section": "",
    "text": "I am currently pursuing a deep learning course as part of my IITM program, complementing my learning journey by creating comprehensive notes and engaging video content. The course content is meticulously curated by me, Hritik Roshan Maurya (HRM), with help of IIT Madras content.\nHaving successfully completed my second year in 2024 with a CGPA of 9.2, I am now delving into deep learning in the January 2025 term. Through this initiative, I aim to document my learning experience while sharing valuable insights with others passionate about this field.",
    "crumbs": [
      "About The Course"
    ]
  },
  {
    "objectID": "classical_perceptron.html",
    "href": "classical_perceptron.html",
    "title": "Classical Perceptron Model",
    "section": "",
    "text": "Proposed by: Frank Rosenblatt (1958)\n\nRefined by: Marvin Minsky & Seymour Papert (1969)\n\nInput: \\(\\mathbb{R}\\), Boolean, etc.\n\nProcessing:\n\ng: Computes the weighted sum of inputs.\nf: Activation function that transforms the output into the required range.\n\nOutput: \\(\\mathbb{R}\\), Boolean, etc.",
    "crumbs": [
      "Week1",
      "Classical Perceptron Model"
    ]
  },
  {
    "objectID": "classical_perceptron.html#classical-perceptron-model",
    "href": "classical_perceptron.html#classical-perceptron-model",
    "title": "Classical Perceptron Model",
    "section": "",
    "text": "Proposed by: Frank Rosenblatt (1958)\n\nRefined by: Marvin Minsky & Seymour Papert (1969)\n\nInput: \\(\\mathbb{R}\\), Boolean, etc.\n\nProcessing:\n\ng: Computes the weighted sum of inputs.\nf: Activation function that transforms the output into the required range.\n\nOutput: \\(\\mathbb{R}\\), Boolean, etc.",
    "crumbs": [
      "Week1",
      "Classical Perceptron Model"
    ]
  },
  {
    "objectID": "classical_perceptron.html#single-perceptron-algorithm",
    "href": "classical_perceptron.html#single-perceptron-algorithm",
    "title": "Classical Perceptron Model",
    "section": "Single Perceptron Algorithm",
    "text": "Single Perceptron Algorithm\nLet’s understand a simple classical perceptron algorithm for classification:\n\nInput: \\(\\mathbb{R}\\)\nOutput: Boolean\n\nGiven inputs (\\(x_1, x_2, ... , x_n\\)), where n is the number of inputs:\n- \\(x_0\\): Bias input (typically set to 1).\n- \\(w_0\\): Bias weight, similar to thresold in McCulloch Pitts Neuron Model.\nThe perceptron rule can be defined mathematically as follows:\n\\[\\begin{align*}\n  y =\n  \\begin{cases}\n    1, & \\text{if } \\sum_{i=0}^n x_i \\cdot w_i \\geq 0 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\end{align*}\\]",
    "crumbs": [
      "Week1",
      "Classical Perceptron Model"
    ]
  },
  {
    "objectID": "classical_perceptron.html#example",
    "href": "classical_perceptron.html#example",
    "title": "Classical Perceptron Model",
    "section": "Example",
    "text": "Example\n\nInput:\n\nisFatherGaveMoney (0, 1): Father gives 40 rupees daily.\nisMotherGaveMoney (0, 1): Mother gives 70 rupees daily.\nbrotherGaveMoney (0, \\(\\infty\\)): Brother gives an unfixed amount of money.\niGiveToJuice (0, 1): If I drink juice, it costs 20 rupees.\n\n\n\nOutput:\n\n0: Did not buy any book.\n1: Bought a book.\n\nThere are 4 books in the store priced at [100, 110, 150, 120].\nI will buy a book if I have enough money collected from my family.\n\n\n\nFraming the Above Problem in Perceptron Form\n\n\nWhat is the bias (\\(w_0\\))?\n\n\nAnswer\n\nAnswer: -100. The bias is 100 because the total money aggregated must be at least 100 to buy any book, as the minimum price of a book is 100.\n\nWhat are the corresponding weights for all inputs?\n\n\nAnswer\n\n\n\\(x_1\\) (isFatherGaveMoney): +40\n\\(x_2\\) (isMotherGaveMoney): +70\n\\(x_3\\)(brotherGaveMoney`): +1\n\\(x_4\\) (iGiveToJuice): -20\n\n\nWhat is the final mathematical function?\n\n\nAnswer\n\n\\(y = \\begin{cases} 1, & \\text{if } -100 + 40x_1 + 70x_2 + x_3 - 20x_4 \\geq 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\)",
    "crumbs": [
      "Week1",
      "Classical Perceptron Model"
    ]
  },
  {
    "objectID": "classical_perceptron.html#lets-understand-working",
    "href": "classical_perceptron.html#lets-understand-working",
    "title": "Classical Perceptron Model",
    "section": "Let’s Understand Working",
    "text": "Let’s Understand Working\nTo better understand how the perceptron works, let’s evaluate it for some sample inputs:\n\n\n\n\n\n\n\n\n\n\n\nisFatherGaveMoney (\\(x_1\\))\nisMotherGaveMoney (\\(x_2\\))\nbrotherGaveMoney (\\(x_3\\))\niGiveToJuice (\\(x_4\\))\nFunction Value (\\(f(x)\\))\nOutput (\\(y\\))\n\n\n\n\n1\n1\n20\n0\n\\(-100 + 40(1) + 70(1) + 1(20) - 20(0) = 30\\)\n1\n\n\n1\n1\n0\n1\n\\(-100 + 40(1) + 70(1) + 1(0) - 20(1) = -10\\)\n0\n\n\n1\n1\n10\n0\n\\(-100 + 40(1) + 70(1) + 1(10) - 20(0) = 20\\)\n1\n\n\n0\n1\n90\n1\n\\(-100 + 40(0) + 70(1) + 1(90) - 20(1) = 40\\)\n1\n\n\n\n\n\nExplanation of the Table:\n\nIn first input, Family gave 130 rs and and after giving 20rs to juce boy, he has 110 rs which crossed the thresold.\nIn Second input, Family given 110 rs. which is enough to buy book. But due to juice, 20 rs deducted and has 90rs only.\nSimilarly all other other cases.",
    "crumbs": [
      "Week1",
      "Classical Perceptron Model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week1 - Perceptron",
    "section": "",
    "text": "Perceptron is the foundation of deep learning.\n\n\nLet’s build intuitions\n\n\nThere are summary notes and other weeks content too.\nUse top document symbol to open navigation bar to navigate to other contents.\n22f3002460@ds.study.iitm.ac.in",
    "crumbs": [
      "Week1"
    ]
  },
  {
    "objectID": "Week/Week2/gd.html",
    "href": "Week/Week2/gd.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "from math import e\ndef f(x,w,b): # sigmoid\n    val = (w*x)+b\n    return 1/(1+e**(-val))\n\n\n\n\ndef dw(w,b):\n    my_sum = 0\n    for x,y in data.items():\n        fval = f(x,w,b)\n        my_sum += (fval-y)*fval*(1-fval)*x\n    return my_sum\n\ndef db(w,b):\n    my_sum = 0\n    for x,y in data.items():\n        fval = f(x,w,b)\n        my_sum += (fval-y)*fval*(1-fval)\n    return my_sum\n\n\n\n\ndef gd(w, b, eta=1, epoch=3):\n    while epoch&gt;0:\n        (w,b) = (w - eta*dw(w,b)/2, b - eta*db(w,b)/2)\n        epoch -= 1\n    return (w,b)\n\n\ndata = {0.5:0.2, 2.5:0.9}\ngd(-2,-2,1,1000)\n\n(1.78460843762914, -2.2720512289757773)",
    "crumbs": [
      "Week2",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "Week/Week2/gd.html#minimize-losswb-sum-frac12fxwb---y2",
    "href": "Week/Week2/gd.html#minimize-losswb-sum-frac12fxwb---y2",
    "title": "Gradient Descent",
    "section": "",
    "text": "from math import e\ndef f(x,w,b): # sigmoid\n    val = (w*x)+b\n    return 1/(1+e**(-val))\n\n\n\n\ndef dw(w,b):\n    my_sum = 0\n    for x,y in data.items():\n        fval = f(x,w,b)\n        my_sum += (fval-y)*fval*(1-fval)*x\n    return my_sum\n\ndef db(w,b):\n    my_sum = 0\n    for x,y in data.items():\n        fval = f(x,w,b)\n        my_sum += (fval-y)*fval*(1-fval)\n    return my_sum\n\n\n\n\ndef gd(w, b, eta=1, epoch=3):\n    while epoch&gt;0:\n        (w,b) = (w - eta*dw(w,b)/2, b - eta*db(w,b)/2)\n        epoch -= 1\n    return (w,b)\n\n\ndata = {0.5:0.2, 2.5:0.9}\ngd(-2,-2,1,1000)\n\n(1.78460843762914, -2.2720512289757773)",
    "crumbs": [
      "Week2",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "Week/Week2/sigmoid.html",
    "href": "Week/Week2/sigmoid.html",
    "title": "Sigmoid Neuron",
    "section": "",
    "text": "\\[\\begin{align*}\n  y =\n    \\begin{cases}\n        1, & \\text{if } w^TX \\geq \\theta \\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{align*}\\]\n\n\n\nOutput: it output only 0 or 1\nWeights: it also says all 0 or 1 with same weight",
    "crumbs": [
      "Week2",
      "Sigmoid Neuron"
    ]
  },
  {
    "objectID": "Week/Week2/sigmoid.html#normal-neuron",
    "href": "Week/Week2/sigmoid.html#normal-neuron",
    "title": "Sigmoid Neuron",
    "section": "",
    "text": "\\[\\begin{align*}\n  y =\n    \\begin{cases}\n        1, & \\text{if } w^TX \\geq \\theta \\\\\n        0, & \\text{otherwise}\n    \\end{cases}\n\\end{align*}\\]\n\n\n\nOutput: it output only 0 or 1\nWeights: it also says all 0 or 1 with same weight",
    "crumbs": [
      "Week2",
      "Sigmoid Neuron"
    ]
  },
  {
    "objectID": "Week/Week2/sigmoid.html#sigmoid-neuron",
    "href": "Week/Week2/sigmoid.html#sigmoid-neuron",
    "title": "Sigmoid Neuron",
    "section": "Sigmoid neuron",
    "text": "Sigmoid neuron\n\\[\\begin{align*}\n  y =\n    \\begin{cases}\n        \\frac{1}{1 + e^{-(w^T X)}}\n    \\end{cases}\n\\end{align*}\\]\n\nHow it is more useful?\n\nOutput: \\(y \\in (0,1)\\)\nWeights: 0.1 can represent strong disagree and 0.4 can represent somewhat agree",
    "crumbs": [
      "Week2",
      "Sigmoid Neuron"
    ]
  },
  {
    "objectID": "Week/Week2/week2.html",
    "href": "Week/Week2/week2.html",
    "title": "Week2 - More On Perceptron",
    "section": "",
    "text": "There are summary notes and other weeks content too.\nUse top document symbol to open navigation bar to navigate to other contents.",
    "crumbs": [
      "Week2"
    ]
  }
]